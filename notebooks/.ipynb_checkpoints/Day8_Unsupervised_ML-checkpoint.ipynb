{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised machine learning techniques\n",
    "\n",
    "Today we will continue working with the gene expression data of our mice, and try to find patterns in them with the help of unsupervised machine learning algorithms. Unsupervised methods are useful when we have so-called unlabeled data: samples with no group membership information, only their raw values.\n",
    "\n",
    "Now our expression data isn't unlabeled: we know for every sample 1) what diet it had, 2) what strain it was. But we will not give that information to the upcoming ML methods. We will instead ask these methods to score / separate / cluster the samples based on their raw values only, and then we will verify whether they managed to do it in a way which is consistent with the labels that we had hidden from the algorithms.\n",
    "\n",
    "The unsupervised techniques we will use today are principal component analysis (PCA), hierarchical clustering and K-means clustering, provided by the wonderful, feature-rich and easy-to-use `scikit-learn` package. Their website is worth taking a look for anyone interested in machine learning: it is not just a documentation, but also a great guide for a lot of techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the expression data\n",
    "\n",
    "We will need the original raw expression data, and your differential expression analysis csv. Adapt the below steps to make sure we are on the same page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.read_excel('../data-livermito/aad0189_DataFileS5.xlsx', header=2)\n",
    "expr = xls.groupby('Gene.1').mean()  # or 'Gene' if you had used that for your DE calculations\n",
    "expr = expr.loc[:, expr.columns.str.contains('Liver')]\n",
    "is_hfd = pd.Series(expr.columns.str.contains('HFD'), index=expr.columns)  # diet labels for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `scikit-learn` likes to treat data as one sample per row, we should transpose our expression data matrix, such that rows stand for mice and columns stand for genes. Let's call the transposed form of `expr` as `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: principal component analysis\n",
    "PCA takes a set of high-dimensional samples (vectors), and transforms them to a smaller set of variables using an orthogonal transformation. It finds a set of orthogonal vectors (their dimension identical to the dimension of your data points) and projects every sample to each of these orthogonal vectors with a simple dot product. The projections are called principal components, and they have some interesting properties that we will not go into detail just yet.\n",
    "\n",
    "### 1.1 Initialize a PCA object with 4 components and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot the components against each other\n",
    "\n",
    "Plotting the first component against the second is simple enough with matplotlib `scatter`, but you can do better and create a scatter plot for each pair using seaborn's `pairplot`. This requires your data to be a `DataFrame`, but you know how to turn a numpy matrix into a DF.\n",
    "\n",
    "### 1.2.1 Add color information based on the diet\n",
    "Are you impressed?\n",
    "\n",
    "### 1.2.2 Optional: is it enough to use every 200th gene and still get a nice PCA plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Find the component vectors that were used for the transformation. Are they orthogonal as promised? How would you verify it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Optional: How correlated are the transformed values with each other? Why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Compare the first component vector's weights with the log-foldchange vector from your differential expression analysis. Visualize them on a scatter plot, and interpret what you see.\n",
    "Remember, you computed the fold-change values by comparing expression values between CD and the HFD diets. PCA had no access to this information, and yet... Well, this is why it's such a popular data exploration technique.\n",
    "\n",
    "You can also try a scatter plot for weight vs p-values, log10-p, etc. Some of them might look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Create a scatter plot for PC1 and PC2 only, but this time connect pairs of points that come from the same strain\n",
    "\n",
    "This will involve `plt.scatter`, and a `for` loop with `plt.plot` calls.\n",
    "The pattern of the resulting lines might be quite interesting. If it is, try to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Hierarchical clustering\n",
    "\n",
    "You had created a cluster-map before (Day 2) and remember those dendrograms on the top and left edges of the figure. They are a result of an unsupervised technique called hierarchical clustering. It iteratively merges single data points into bigger and bigger clusters based on their similarity, one at a time, until all points belong to one big cluster. Seaborn's `clustermap` does it on both axes by default, and produces a heatmap of the values as well.\n",
    "\n",
    "### 2.1 Create a clustermap\n",
    "From your first PCA component vector, take the genes with the 15 largest positive, and 15 largest negative weights, and create a smaller expression matrix with these 30 genes only. Use seaborn's `clustermap` to do a hierarchical clustering of genes and mice alike, as well as a heatmap.\n",
    "\n",
    "\n",
    "### 2.2 Display the diet at the bottom of the mice's dendrogram\n",
    "You will have to convert `is_hfd`'s values to color values first: `y` or `yellow` for 0 and `b` or `blue` for 1 should do fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Standardize the matrix\n",
    "It is hard to see the fine differences between the gene expression levels acros mice, because the range of general expression levels across genes is much higher. To circumvent this, we usually standardize our data: for every gene, subtract the mean and divide by the standard deviation.\n",
    "\n",
    "You can do this with a pandas one-liner, or use sklearn's `StandardScaler` tool. You can simply extend the above cell with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Try different linkage methods\n",
    "The linkage method defines how cluster similarities are defined. For example, `single` defines the distance of two clusters as the smallest distance of any two elements between them. Its opposite is `complete` which takes the largest distance of any two elements across two clusters. Middle grounds are `centroid`, `average`, and a few more. Look at how they affect the topology of the clusters. Which one seems most suitable in our case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: K-means clustering\n",
    "\n",
    "K-means clustering attempts to create `k` virtual samples, whose average distance to the nearest actual samples is as small as possible. These `k` virtual samples are called cluster centers/prototypes or centroids. Each sample is assigned to the nearest centroid, therefore partitioning the samples to k clusters.\n",
    "\n",
    "### 3.1 Perform k-means clustering on the expression dataset. What k should you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare the resulting clusters with the diet labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optional: with k=2, take the centroids, and transform them with the same PCA that you had trained earlier. Mark them on the PC1 vs PC2 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train a 2-means clustering on every second sample and predict on the other half of the samples\n",
    "Since our CD and HFD samples come in two big batches, you can just use `::2` and `1::2` to split them. What do you find?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
